{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No module named 'pyopencl'. cl12 version by Christoph Gohlke for windows is recommended: https://www.lfd.uci.edu/~gohlke/pythonlibs/#pyopencl\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gc\n",
    "import time\n",
    "import datetime\n",
    "from PIL import Image\n",
    "from SSIM_PIL import compare_ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_layer(tensor_in, name_layer, is_training, f_num, f_size):\n",
    "    x = tf.layers.conv2d(\n",
    "        inputs = tensor_in,\n",
    "        filters = f_num,\n",
    "        kernel_size = [filter_size_in, filter_size_in],\n",
    "        padding = \"same\",\n",
    "        activation= None,\n",
    "        name = name_layer,\n",
    "        use_bias=False)\n",
    "    \n",
    "    x = tf.layers.batch_normalization(x, name = name_layer + \"_bn\",\n",
    "                                             center=True, \n",
    "                                             scale=True, \n",
    "                                             training=is_training)\n",
    "    \n",
    "    return tf.nn.relu(x, name = name_layer + \"_relu\")\n",
    "def conv_layer_without_relu(tensor_in, name_layer, is_training, f_num, f_size):\n",
    "    x = tf.layers.conv2d(\n",
    "        inputs = tensor_in,\n",
    "        filters = f_num,\n",
    "        kernel_size = [filter_size_in, filter_size_in],\n",
    "        padding = \"same\",\n",
    "        activation= None,\n",
    "        name = name_layer,\n",
    "        use_bias=False)\n",
    "    \n",
    "    x = tf.layers.batch_normalization(x, name = name_layer + \"_bn\",\n",
    "                                             center=True, \n",
    "                                             scale=True, \n",
    "                                             training=is_training)\n",
    "    \n",
    "    return x\n",
    "def DnCNN_model(features, labels, mode):\n",
    "   # Input Layer\n",
    "    input_oiginal = features['x']\n",
    "   \n",
    "    # Convolutional layer #1     \n",
    "    input_layer = tf.layers.conv2d(\n",
    "        inputs = input_oiginal,\n",
    "        filters = filter_num_in,\n",
    "        kernel_size = filter_size_in,\n",
    "        padding = \"same\",\n",
    "        activation= tf.nn.relu,\n",
    "        name = \"Conv_Init\")\n",
    "    is_training_mode = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    \n",
    "    if (skipped_scheme_in == \"ResNet\"):\n",
    "        # Standard DnCNN skipped layers scheme: one residual link between first and last\n",
    "        cur_tensor = input_layer\n",
    "        for i in range(1, depth_in + 1):\n",
    "            if i == 1:\n",
    "                cur_tensor = conv_layer(cur_tensor, \"conv{}\".format(i), is_training_mode, filter_num_in, filter_size_in)\n",
    "                save = cur_tensor\n",
    "            elif (i - 1) % 2 == 0:\n",
    "                cur_tensor = conv_layer_without_relu(cur_tensor, \"conv{}\".format(i), is_training_mode, filter_num_in, filter_size_in)\n",
    "                cur_tensor += save\n",
    "                cur_tensor = tf.nn.relu(cur_tensor, name = \"conv{}\".format(i) + \"_relu\")\n",
    "                save = cur_tensor\n",
    "            else:\n",
    "                cur_tensor = conv_layer(cur_tensor, \"conv{}\".format(i), is_training_mode, filter_num_in, filter_size_in)\n",
    "                \n",
    "        # Final layers: Make it a gray scale image again!\n",
    "        final_layer = tf.layers.conv2d(\n",
    "        inputs = cur_tensor + save,\n",
    "        filters = 1,\n",
    "        kernel_size = [1, 1],\n",
    "        padding = \"same\",\n",
    "        activation = None,\n",
    "        name = \"final_layer\")\n",
    "\n",
    "    else:\n",
    "        # Standard DnCNN skipped layers scheme: one residual link between first and last\n",
    "        cur_tensor = input_layer\n",
    "        for i in range(1, depth_in + 1):\n",
    "            cur = conv_layer(cur_tensor, \"conv{}\".format(i), is_training_mode, filter_num_in, filter_size_in)\n",
    "\n",
    "        # Final layers: Make it a gray scale image again!\n",
    "        final_layer = tf.layers.conv2d(\n",
    "            inputs = cur_tensor + input_layer,\n",
    "            filters = 1,\n",
    "            kernel_size = [1, 1],\n",
    "            padding = \"same\",\n",
    "            activation = None,\n",
    "            name = \"final_layer\") \n",
    "      \n",
    "    # Give output in prediction mode\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode = mode, predictions=final_layer)\n",
    "\n",
    "    # Calulate loss function according to loss_scheme\n",
    "    if(loss_scheme_in == 'SSIM'):\n",
    "        loss = -tf.reduce_mean(tf.image.ssim(labels, final_layer, 1.0))\n",
    "    elif(loss_scheme_in == 'L1'):\n",
    "        loss = tf.losses.mean_squared_error(labels = labels , predictions = final_layer)\n",
    "    else:\n",
    "        loss =  loss = tf.losses.absolute_difference(labels = labels , predictions = final_layer)\n",
    "       \n",
    "    ### Print summary ###\n",
    "    if (mode != tf.estimator.ModeKeys.PREDICT):\n",
    "        tf.summary.image(\"Undersampled\", input_oiginal, max_outputs = 1)\n",
    "        tf.summary.image(\"Output_Image\", final_layer, max_outputs = 1)\n",
    "        tf.summary.image(\"True_Image\", labels,  max_outputs = 1)\n",
    "        tf.summary.histogram(\"Summary_final_layer\", final_layer)\n",
    "        tf.summary.histogram(\"Summary_labels\", labels)\n",
    "        tf.summary.scalar(\"Value_Loss_Function\", loss)\n",
    "        \n",
    "        # Besides the specific ones write out all trainiable variables\n",
    "        for var in tf.trainable_variables():\n",
    "            name = var.name\n",
    "            name = name.replace(':', '_')\n",
    "            tf.summary.histogram(name, var)\n",
    "            \n",
    "        # Finally merge the summary\n",
    "        merged_summary = tf.summary.merge_all()\n",
    "\n",
    "    # Specify Learning \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        # This is needed so that Batch normalization paramters are trained as well.\n",
    "        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(update_ops):\n",
    "            # Specify the Optimizer\n",
    "            original_optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate_in)\n",
    "            # Use gradient clipping to avoid exploding gradients\n",
    "            optimizer = tf.contrib.estimator.clip_gradients_by_norm(original_optimizer, clip_norm=5.0)\n",
    "            # The loss function should be minimized.\n",
    "            train_op = optimizer.minimize(loss = loss, global_step=tf.train.get_global_step())\n",
    "            return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(directory, name, model_id, batch_size, steps):\n",
    "    runconf = tf.estimator.RunConfig(save_summary_steps=5, log_step_count_steps = 10, tf_random_seed= 1993)\n",
    "    DnCNN = tf.estimator.Estimator(config=runconf,\n",
    "        model_fn=DnCNN_model, model_dir = directory + name + \"_\" + model_id)\n",
    "\n",
    "    train = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={\"x\": X_train},\n",
    "        y=Y_train,\n",
    "        batch_size=batch_size,\n",
    "        num_epochs=None,\n",
    "        shuffle=True)\n",
    "    \n",
    "    DnCNN.train(input_fn=train, steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(directory, name, model_id, batch_size, steps):\n",
    "    # Specify Model\n",
    "    runconf = tf.estimator.RunConfig(save_summary_steps=1000, log_step_count_steps = 1000, tf_random_seed= 1993)\n",
    "    DnCNN = tf.estimator.Estimator(config=runconf,\n",
    "        model_fn=DnCNN_model, model_dir = directory + name + \"_\" + model_id)\n",
    "    \n",
    "    # Evaluate \n",
    "    predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={\"x\": X_test[:,:,:,:]},\n",
    "        y=Y_test[:,:,:,:],\n",
    "        batch_size = 1,\n",
    "        shuffle=False)\n",
    "    predict_results = DnCNN.predict(input_fn=predict_input_fn)\n",
    "    \n",
    "    sum_mae = 0\n",
    "    sum_mse = 0\n",
    "    sum_ssim = 0\n",
    "    \n",
    "    #for im_num in range(0, Y_test.shape[0]):\n",
    "    for im_num in range(0, 10):\n",
    "        prediction = next(predict_results)\n",
    "        true_image = Y_test[im_num,:,:,:]\n",
    "        sum_mae += np.mean(np.abs(prediction - true_image))\n",
    "        sum_mse += np.mean(np.power((prediction - true_image), 2))\n",
    "        sum_ssim += compare_ssim(Image.fromarray((prediction[:,:,0] * 255).astype('uint8'), 'L'),\n",
    "                 Image.fromarray((true_image[:,:,0] * 255).astype('uint8'), 'L'))\n",
    "    \n",
    "    mean_mae = sum_mae/ X_train.shape[0]\n",
    "    mean_mse = sum_mse / X_train.shape[0]\n",
    "    mean_ssim = sum_ssim / X_train.shape[0]\n",
    "    return([mean_mae, mean_mse, mean_ssim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Execute_model(depth,\n",
    "                  filter_num,\n",
    "                  loss_scheme,\n",
    "                  skipped_scheme,\n",
    "                  filter_size,\n",
    "                  learning_rate,\n",
    "                  directory,\n",
    "                  model_id,\n",
    "                  name,\n",
    "                  batch_size,\n",
    "                  steps):\n",
    "    # Check arguments\n",
    "    if (depth < 1 or depth > 40):\n",
    "        raise Exception('Depth of {} is not allowed!'.format(depth))\n",
    "    if(filter_num < 1 or filter_num > 256):\n",
    "        raise Exception('filter_num of {} is not allowed!'.format(filter_num))\n",
    "    if(not (loss_scheme == \"L1\" or loss_scheme == \"L2\" or loss_scheme == \"SSIM\")):\n",
    "        raise Exception('loss_scheme {} is not allowed!'.format(loss_scheme))\n",
    "    if(not (skipped_scheme == 'DnCNN' or skipped_scheme == 'ResNet')):\n",
    "        raise Exception('skipped_scheme {} is not allowed!'.format(skipped_scheme))\n",
    "    if(filter_size < 1 or filter_size > 9):\n",
    "        raise Exception('filter_size of {} is not allowed!'.format(filter_size))\n",
    "    if(learning_rate < 0 or learning_rate > 1):\n",
    "        raise Exception('learning_rate of {} is not allowed!'.format(learning_rate))\n",
    "        sum_ssim\n",
    "        \n",
    "    # Define arguments global\n",
    "    global depth_in, filter_num_in, loss_scheme_in, skipped_scheme_in, filter_size_in, learning_rate_in, directory_in, model_id_in, name_in, batch_size_in, steps_in\n",
    "    depth_in = depth\n",
    "    filter_num_in = filter_num\n",
    "    loss_scheme_in = loss_scheme\n",
    "    skipped_scheme_in = skipped_scheme\n",
    "    filter_size_in = filter_size\n",
    "    learning_rate_in = learning_rate\n",
    "    directory_in = directory\n",
    "    model_id_in = model_id\n",
    "    name_in = name\n",
    "    batch_size_in = batch_size\n",
    "    steps_in = steps\n",
    "        \n",
    "    \n",
    "    ## run model ##\n",
    "    Runtime_test = -1.0\n",
    "    Runtime_train = -1.0\n",
    "    start = time.time()\n",
    "    error_message = ' '\n",
    "    try:\n",
    "        run_model(directory, name, model_id, batch_size, steps)\n",
    "        Runtime_train = time.time() - start\n",
    "        start = time.time()\n",
    "        try:\n",
    "            eval_results = eval_model(directory, name, model_id, batch_size, steps)\n",
    "            MAE = eval_results[0]\n",
    "            MSE = eval_results[1]\n",
    "            SSIM = eval_results[2]\n",
    "        except Exception as e:\n",
    "            print(\"Error in testing. \" + str(e))\n",
    "            error_message =  \"Error in testing. \" + str(e)\n",
    "            MAE = -10.0\n",
    "            MSE =  -10.0\n",
    "            SSIM =  -10.0\n",
    "        Runtime_test = time.time() - start\n",
    "    except Exception as e:\n",
    "        print(\"Error in trainnig. \" + str(e))\n",
    "        error_message = \"Error in trainnig. \" + str(e)\n",
    "        MAE = -10.0\n",
    "        MSE = -10.0\n",
    "        SSIM = -10.0\n",
    "        \n",
    "    # Remove global variables\n",
    "    del depth_in, filter_num_in, loss_scheme_in, skipped_scheme_in, filter_size_in, learning_rate_in, directory_in, model_id_in, name_in, batch_size_in, steps_in\n",
    "    if error_message == ' ':\n",
    "        error_message = \"No Error occured.\"\n",
    "    # return MAE, MSE, SSIM on test and run-time\n",
    "    return({'Avg_MAE': MAE, 'Avg_MSE': MSE, 'Avg_SSIM' : SSIM, 'Runtime_test' : Runtime_test, 'Runtime_train' : Runtime_train, \"Error_message\" : error_message})   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_grid(grid_loc,  #Location of the grid to run\n",
    "             directory, #Location of the models that should be run \n",
    "             X_train_loc, # Location of the X training set\n",
    "             Y_train_loc, # Location of the Y training set\n",
    "             X_test_loc, # Location of the X test set\n",
    "             Y_test_loc # Location of the Y test set\n",
    "                  name,  #Names of the models\n",
    "                  steps, # How many steps should each model be run\n",
    "                  batch_size # With what batch sizes should the model be runned\n",
    "                    ):\n",
    "    \n",
    "    # Load grid to run\n",
    "    data = pd.read_csv(grid_loc, dtype={'Error_message': str, 'location': str})\n",
    "    \n",
    "    # Load data for model\n",
    "    #X_train = np.load('/scratch2/ttoebro/data/X_test_pois_1_9.npy')\n",
    "    #Y_train = np.load('/scratch2/ttoebro/data/Y_test_pois_1_9.npy')\n",
    "    #X_test = np.load('/scratch2/ttoebro/data/X_test_pois_1_9.npy')\n",
    "    #Y_test = np.load('/scratch2/ttoebro/data/Y_test_pois_1_9.npy')\n",
    "    \n",
    "    # Run each model specification in the rows if the 'location' columns is empty\n",
    "    for row in range(0, data.shape[0]):\n",
    "        if(data.loc[row, 'location'] == \"not_run\"):\n",
    "            e = ''\n",
    "            # Run Model specification\n",
    "            try:\n",
    "                res = Execute_model(data.loc[row, 'depth'],\n",
    "                      data.loc[row, 'filter_num'],\n",
    "                      data.loc[row, 'loss_scheme'],\n",
    "                      data.loc[row, 'skipped_scheme'],\n",
    "                      int(data.loc[row, 'filter_size']),\n",
    "                      data.loc[row, 'learning_rate'],\n",
    "                      directory,\n",
    "                      str(row),\n",
    "                      name,\n",
    "                      batch_size,\n",
    "                              steps)\n",
    "            except Exception as e:\n",
    "                print(\"Error in Execute model. \" + str(e))\n",
    "                e = \"Error in Execute model. \" + str(e)\n",
    "                data.loc[row, 'Avg_MAE'] = -12.0\n",
    "                data.loc[row, 'Avg_MSE'] = -12.0\n",
    "                data.loc[row, 'Avg_SSIM'] = -12.0\n",
    "                data.loc[row, 'Runtime_test'] = -3.0\n",
    "                data.loc[row, 'Runtime_train'] = -3.0\n",
    "                data.loc[row, 'Error_message'] = e\n",
    "                data.loc[row, 'location'] = '-.-'\n",
    "                continue\n",
    "                \n",
    "            data.loc[row, 'Avg_MAE'] = res['Avg_MAE']\n",
    "            data.loc[row, 'Avg_MSE'] = res['Avg_MSE']\n",
    "            data.loc[row, 'Avg_SSIM'] = res['Avg_SSIM']\n",
    "            data.loc[row, 'Runtime_test'] = res['Runtime_test']\n",
    "            data.loc[row, 'Runtime_train'] = res['Runtime_train']\n",
    "            data.loc[row, 'Error_message'] = res['Error_message']\n",
    "            data.loc[row, 'location'] = directory + name + \"_\" + str(row)\n",
    "            # Save results from run to data\n",
    "          \n",
    "            # Save progress\n",
    "            data.to_csv(grid_loc, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('/scratch2/ttoebro/data/X_test_pois_1_9.npy')\n",
    "Y_train = np.load('/scratch2/ttoebro/data/Y_test_pois_1_9.npy')\n",
    "X_test = np.load('/scratch2/ttoebro/data/X_test_pois_1_9.npy')\n",
    "Y_test = np.load('/scratch2/ttoebro/data/Y_test_pois_1_9.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '/scratch2/ttoebro/Grid_search/ModelsDnCNN_0', '_tf_random_seed': None, '_save_summary_steps': 5, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 10, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f47a4703d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /scratch2/ttoebro/Grid_search/ModelsDnCNN_0/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0029150706, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 10 into /scratch2/ttoebro/Grid_search/ModelsDnCNN_0/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.029993.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/scratch2/ttoebro/Grid_search/ModelsDnCNN_0', '_tf_random_seed': None, '_save_summary_steps': 1000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f47a4703a20>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /scratch2/ttoebro/Grid_search/ModelsDnCNN_0/model.ckpt-10\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/scratch2/ttoebro/Grid_search/ModelsDnCNN_1', '_tf_random_seed': None, '_save_summary_steps': 5, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 10, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f476c7efc50>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /scratch2/ttoebro/Grid_search/ModelsDnCNN_1/model.ckpt.\n",
      "INFO:tensorflow:loss = 23.967129, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 10 into /scratch2/ttoebro/Grid_search/ModelsDnCNN_1/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.28847024.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/scratch2/ttoebro/Grid_search/ModelsDnCNN_1', '_tf_random_seed': None, '_save_summary_steps': 1000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f476c7ef0b8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /scratch2/ttoebro/Grid_search/ModelsDnCNN_1/model.ckpt-10\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/scratch2/ttoebro/Grid_search/ModelsDnCNN_2', '_tf_random_seed': None, '_save_summary_steps': 5, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 10, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f47657160b8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /scratch2/ttoebro/Grid_search/ModelsDnCNN_2/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.13637903, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 10 into /scratch2/ttoebro/Grid_search/ModelsDnCNN_2/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.053197533.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/scratch2/ttoebro/Grid_search/ModelsDnCNN_2', '_tf_random_seed': None, '_save_summary_steps': 1000, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 1000, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f47d2e980b8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /scratch2/ttoebro/Grid_search/ModelsDnCNN_2/model.ckpt-10\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "run_grid('/scratch2/ttoebro/Grid_search/Grid/main_grid.csv',  '/scratch2/ttoebro/Grid_search/Models/',  'DnCNN',  10, 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Execute_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-52196d373966>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m Execute_model(depth = 6,\n\u001b[0m\u001b[1;32m      2\u001b[0m                   \u001b[0mfilter_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                   \u001b[0mloss_scheme\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'L1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                   \u001b[0mskipped_scheme\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ResNet'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                   \u001b[0mfilter_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Execute_model' is not defined"
     ]
    }
   ],
   "source": [
    "Execute_model(depth = 6,\n",
    "                  filter_num = 16,\n",
    "                  loss_scheme = 'L1',\n",
    "                  skipped_scheme = 'ResNet',\n",
    "                  filter_size = 5,\n",
    "                  learning_rate = 0.05,\n",
    "                  directory = \"/scratch2/ttoebro/models/test/\",\n",
    "                  name = \"DnCNN\",\n",
    "                  model_id = \"test\",\n",
    "                  batch_size = 8,\n",
    "                  steps = 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
